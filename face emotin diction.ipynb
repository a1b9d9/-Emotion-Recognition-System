{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import os\n",
    "import cv2\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import moviepy.editor\n",
    "import librosa\n",
    "import soundfile\n",
    "import os, glob, pickle\n",
    "import numpy as np\n",
    "from librosa.core import istft\n",
    "from tensorflow import keras\n",
    "#model = model_from_json(open(\"fer.json\", \"r\").read())\n",
    "#model.load_weights('fer.h5') \n",
    "#face_haar_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')    \n",
    "Model_Emotion_V = keras.models.load_model('V_E_R\\\\V_E_R.h5')\n",
    "#model = model_from_json(open(\"fer.json\", \"r\").read())\n",
    "model= load_model('F_E_R\\\\simple_CNN.530-0.65.hdf5')\n",
    "face_haar_cascade = cv2.CascadeClassifier('F_E_R\\\\haarcascade_frontalface_default.xml')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(file_name, mfcc, chroma, mel):\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        X = librosa.to_mono(X)\n",
    "        sample_rate=sound_file.samplerate\n",
    "        if chroma:\n",
    "            stft=np.abs(librosa.core.stft(X))\n",
    "        result=np.array([])\n",
    "        if mfcc:\n",
    "            mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "            result=np.hstack((result, mfccs))\n",
    "        if chroma:\n",
    "            chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "            result=np.hstack((result, chroma))\n",
    "        if mel:\n",
    "            mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "            result=np.hstack((result, mel))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFlair - Load the data and extract features for each sound file\n",
    "def load_data(path):\n",
    " #   file in glob.glob(path)\n",
    "    x=[]\n",
    "    feature=extract_feature(path, mfcc=True, chroma=True, mel=True)\n",
    "    feature = np.reshape(feature,(6,30))\n",
    "    feature=feature.reshape(1, feature.shape[0], feature.shape[1], 1)\n",
    " \n",
    "    predictions = Model_Emotion_V.predict(feature)\n",
    "    max_index = numpy.argmax(predictions[0])\n",
    "    emotion_detection = ('angry','fear','happy','neutral','sad')\n",
    "    emotion_prediction = emotion_detection[max_index]\n",
    "    return emotion_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_F_R(path):\n",
    "    \n",
    "    video =  moviepy.editor.VideoFileClip(path)\n",
    "    video.audio.write_audiofile(r\"output.wav\")\n",
    "    \n",
    "    \n",
    "    cap = cv2.VideoCapture(path)\n",
    "    hap=nat=ang=far=sad=0\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "                break\n",
    "\n",
    "            #Change the frame to greyscale  \n",
    "        gray_image= cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            #We pass the image, scaleFactor and minneighbour\n",
    "        faces_detected = face_haar_cascade.detectMultiScale(gray_image,1.32,5)\n",
    "\n",
    "            #Draw Triangles around the faces detected\n",
    "        for (x,y,w,h) in faces_detected:\n",
    "            cv2.rectangle(frame,(x,y), (x+w,y+h), (255,0,0), thickness=7)\n",
    "            roi_gray=gray_image[y:y+w,x:x+h]\n",
    "            roi_gray=cv2.resize(roi_gray,(48,48))\n",
    "\n",
    "                #Processes the image and adjust it to pass it to the model\n",
    "            image_pixels = tf.keras.preprocessing.image.img_to_array(roi_gray)\n",
    "              #  plt.imshow(image_pixels)\n",
    "               # plt.show()\n",
    "            image_pixels = np.expand_dims(image_pixels, axis = 0)\n",
    "            image_pixels /= 255\n",
    "\n",
    "                #Get the prediction of the model\n",
    "            predictions = model.predict(image_pixels)\n",
    "         #   print(predictions)\n",
    "            max_index = np.argmax(predictions[0])\n",
    "            emotion_detection = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "            emotion_prediction = emotion_detection[max_index]\n",
    "            if emotion_prediction==\"happy\" :\n",
    "                hap=hap+1\n",
    "            if emotion_prediction==\"angry\" :\n",
    "                ang=ang+1\n",
    "            if emotion_prediction==\"fear\" :\n",
    "                far=far+1\n",
    "            if emotion_prediction==\"sad\" :\n",
    "                sad=sad+1\n",
    "            if emotion_prediction==\"neutral\" :\n",
    "                nat=nat+1\n",
    "\n",
    "                #Write on the frame the emotion detected\n",
    "    #        cv2.putText(frame,emotion_prediction,(int(x), int(y)),cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)\n",
    "\n",
    "\n",
    "    #        resize_image = cv2.resize(frame, (700, 700))\n",
    "    #        cv2.imshow('Emotion',resize_image)\n",
    "            if cv2.waitKey(10) == ord('b'):\n",
    "                    break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows\n",
    "    \n",
    "\n",
    "    emotion_detection = ('angry', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "    a=[ang,far,hap,sad,nat]\n",
    "    k=emotion_detection[np.argmax(a)]\n",
    "    Index=np.argmax(a)\n",
    "    path=\"output.wav\"\n",
    "    V_E=load_data(path)\n",
    "    os.remove(\"output.wav\")\n",
    "    \n",
    "    if k!=V_E:\n",
    "        M_E=k\n",
    "        a[Index]=0\n",
    "        k=emotion_detection[np.argmax(a)]\n",
    "        if k!=V_E:\n",
    "            k=M_E\n",
    "\n",
    "    \n",
    "        \n",
    "    \n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "chunk:   0%|                                                                         | 0/180 [00:00<?, ?it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in output.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'happy'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p='C:\\\\Users\\\\abd alrhman\\\\Downloads\\\\٢٠٢١٠٧٢٤_١٨٥٧٣٧.mp4'\n",
    "\n",
    "E_F_R(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
