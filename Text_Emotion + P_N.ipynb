{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize  \n",
    "from ar_corrector.corrector import Corrector\n",
    "corr = Corrector()\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "from nltk import ngrams\n",
    "import numpy as np\n",
    "import re\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_file = open(\"Text_Emotion\\\\dict_Emotion_N.pkl\", \"rb\")\n",
    "dict_Emotion_N = pickle.load(a_file)\n",
    "\n",
    "a_file = open(\"Text_Emotion\\\\dict_Emotion_N.pkl\", \"rb\")\n",
    "dict_Emotion_HS = pickle.load(a_file)\n",
    "\n",
    "a_file = open(\"Text_Emotion\\\\dict_Emotion_N.pkl\", \"rb\")\n",
    "dict_Emotion_AF = pickle.load(a_file)\n",
    "\n",
    "\n",
    "feature_path = 'Text_Emotion\\\\feature.pkl'\n",
    "loaded_vec = CountVectorizer(decode_error=\"replace\", vocabulary=pickle.load(open(feature_path, \"rb\")))\n",
    "# Load TfidfTransformer\n",
    "tfidftransformer_path = 'Text_Emotion\\\\tfidftransformer.pkl'\n",
    "tfidftransformer = pickle.load(open(tfidftransformer_path, \"rb\"))\n",
    "\n",
    "\n",
    "Model_Emotion = keras.models.load_model('Text_Emotion\\\\model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(text):\n",
    "  search = [\"ÿ£\",\"ÿ•\",\"ÿ¢\",\"ÿ©\",\"_\",\"-\",\"/\",\".\",\"ÿå\",\" Ÿà \",\" Ÿäÿß \",'\"',\"ŸÄ\",\"'\",\"Ÿâ\",\n",
    "              \"\\\\\",'\\n', '\\t','&quot;','?','ÿü','!']\n",
    "  replace = [\"ÿß\",\"ÿß\",\"ÿß\",\"Ÿá\",\" \",\" \",\"\",\"\",\"\",\" Ÿà\",\" Ÿäÿß\",\n",
    "               \"\",\"\",\"\",\"Ÿä\",\"\",' ', ' ',' ',' ? ',' ÿü ', ' ! ']\n",
    "  longation = re.compile(r'(.)\\1+')\n",
    "  subst = r\"\\1\\1\"\n",
    "  text = re.sub(longation, subst, text)\n",
    "  text = re.sub(r\"[a-zA-Z]\", '', text)\n",
    "  text = re.sub(r\"\\d+\", ' ', text)\n",
    "  text = re.sub(r\"\\n+\", ' ', text)\n",
    "  text = re.sub(r\"\\t+\", ' ', text)\n",
    "  text = re.sub(r\"\\r+\", ' ', text)\n",
    "  text = re.sub(r\"\\s+\", ' ', text)\n",
    "  text = re.sub(\"ÿ£+\", \"ÿß\", text)\n",
    "  text = re.sub(\"ÿßÿß+\", \"ÿß\", text)\n",
    "  text = re.sub(\"ŸàŸà+\", \"Ÿà\", text)\n",
    "  text = re.sub(\"ŸäŸä+\", \"Ÿä\", text)\n",
    "  text = re.sub(\"ŸáŸá+\", \"Ÿá\", text)\n",
    "  text = re.sub(\"ü§£ü§£+\",\"ü§£\", text)\n",
    "  text = re.sub(\"üòÇüòÇ+\",\"üòÇ\", text)\n",
    "  text = re.sub(\"üò≠üò≠+\",\"üò≠\", text)\n",
    "  text = re.sub(\"üò±üò±+\",\"üò±\", text)\n",
    "  text = re.sub(\"üò°üò°+\",\"üò°\", text)\n",
    "  text = re.sub(\"üòÄüòÄ+\",\"üòÄ\", text)\n",
    "\n",
    "\n",
    "  for i in range(0, len(search)):\n",
    "     text = text.replace(search[i], replace[i])\n",
    "    \n",
    "  text = text.strip()\n",
    " # text=corr.contextual_correct(text)\n",
    "  stop_word = open('Text_Emotion\\\\stop word.txt',encoding=\"utf8\")\n",
    "  stop_word=stop_word.read()\n",
    "  remov=list()\n",
    "  terms= word_tokenize(text)\n",
    "\n",
    "\n",
    "  for i in range(len(terms)):\n",
    "        if terms[i] in stop_word:\n",
    "            remov.append(terms[i])\n",
    "            \n",
    "            \n",
    "  for i in remov:\n",
    "    terms.remove(i)\n",
    "  cancot=\"\"\n",
    "  for i in terms:\n",
    "        if cancot==\"\":\n",
    "            cancot=i\n",
    "        else:\n",
    "            cancot=cancot+\" \"+i\n",
    "  text=cancot          \n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÿ™ÿπÿØŸäŸÑ ŸáŸàŸÜ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prper_data(text,n=3):\n",
    "#n  number of gram\n",
    "\n",
    "    Link=numpy.zeros((4,151))\n",
    "    vec_emotion_tfidf=vec_emotion_N=vec_emotion_AF=vec_emotion_HS=Cnn_F=list()\n",
    "    is_zero=0\n",
    "    sentence = clean_data(text)\n",
    "    tfidf = tfidftransformer.transform(loaded_vec.transform([sentence]))\n",
    "    tfidf=tfidf.toarray()\n",
    "\n",
    "    \n",
    "    for k in range(n):\n",
    "        \n",
    "        term_ngrams = ngrams(sentence.split(), (k+1))\n",
    "        term_ngrams=list(term_ngrams)\n",
    "        for kk in range(len(term_ngrams)):\n",
    "            if n !=0:\n",
    "                word=\"\"\n",
    "                for kkk in range(len(term_ngrams[kk])):\n",
    "                    if kkk == 0:\n",
    "                        word = term_ngrams[kk][kkk]\n",
    "                    else:\n",
    "                        word = word +\" \"+ term_ngrams[kk][kkk]\n",
    "                        \n",
    "            else:\n",
    "                word=term_ngrams[kk][0]\n",
    "            \n",
    "            index_sentence=loaded_vec.vocabulary_.get(word)\n",
    "            \n",
    "            if index_sentence ==None:\n",
    "                T_F=0\n",
    "            else:\n",
    "                T_F=tfidf[0][index_sentence]  \n",
    "                \n",
    "            E_N=dict_Emotion_N.get(word)\n",
    "            E_AF=dict_Emotion_AF.get(word)\n",
    "            E_HS=dict_Emotion_HS.get(word)\n",
    "\n",
    "            \n",
    "            if E_N ==None:\n",
    "                E_N=0\n",
    "            if E_AF ==None:\n",
    "                E_AF=0\n",
    "            if E_HS ==None:\n",
    "                E_HS=0  \n",
    "                \n",
    "            vec_emotion_tfidf.append(T_F)\n",
    "            vec_emotion_N.append(E_N)\n",
    "            vec_emotion_AF.append(E_AF)\n",
    "            vec_emotion_HS.append(E_HS)\n",
    "            \n",
    "            \n",
    "    for e1 in range(len(vec_emotion_tfidf)):\n",
    "        if e1>150:\n",
    "            break\n",
    "        else:\n",
    "            Link[0][e1]=vec_emotion_tfidf[e1]\n",
    "    \n",
    "    \n",
    "    is_all_zero = np.all((Link[0] == 0))\n",
    "    if is_all_zero:\n",
    "        is_zero=is_zero+1\n",
    "        \n",
    "        \n",
    "        \n",
    "    for e1 in range(len(vec_emotion_N)):\n",
    "        if e1>150:\n",
    "            break\n",
    "        else:\n",
    "            Link[1][e1]=vec_emotion_N[e1]\n",
    "        \n",
    "    is_all_zero = np.all((Link[1] == 0))\n",
    "    if is_all_zero:\n",
    "        is_zero=is_zero+1\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "    for e1 in range(len(vec_emotion_AF)):\n",
    "        if e1>150:\n",
    "            break\n",
    "        else:\n",
    "            Link[2][e1]=vec_emotion_AF[e1]\n",
    "        \n",
    "    is_all_zero = np.all((Link[2] == 0))\n",
    "    if is_all_zero:\n",
    "        is_zero=is_zero+1\n",
    "        \n",
    "       \n",
    "        \n",
    "    for e1 in range(len(vec_emotion_HS)):\n",
    "        if e1>150:\n",
    "            break\n",
    "        else:\n",
    "            Link[3][e1]=vec_emotion_HS[e1]\n",
    "        \n",
    "    is_all_zero = np.all((Link[3] == 0))\n",
    "    if is_all_zero:\n",
    "        is_zero=is_zero+1\n",
    "        \n",
    "       \n",
    "    if is_zero ==0:\n",
    "        Link = Link.reshape(1, Link.shape[0], Link.shape[1], 1)  \n",
    "    else:\n",
    "        Link=\"N\"\n",
    "    return Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_emotion(text):\n",
    "    text=prper_data(text)\n",
    "    emotion_prediction=\"\"\n",
    "    if text!= \"N\":\n",
    "        predictions = Model_Emotion.predict(text)\n",
    "        max_index = numpy.argmax(predictions[0])\n",
    "        emotion_detection = ('angry', 'sad', 'fear', 'neutral', 'happy')\n",
    "        emotion_prediction = emotion_detection[max_index]\n",
    "    else:\n",
    "        emotion_prediction=\"Neutral\"\n",
    "    return emotion_prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-92-e33c0f606b6d>:4: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if text!= \"N\":\n"
     ]
    }
   ],
   "source": [
    "text=\"ÿßŸÑŸÑŸá ŸÑÿß ŸäŸàŸÅŸÇŸÉ\"\n",
    "text=model_emotion(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ÿßŸÜÿ™ÿ≠ÿßÿ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_file = open(\"P_N\\\\dict_P_N.pkl\", \"rb\")\n",
    "dict_P_N = pickle.load(a_file)\n",
    "\n",
    "\n",
    "\n",
    "feature_path_P = 'P_N\\\\featureP.pkl'\n",
    "loaded_vec_P = CountVectorizer(decode_error=\"replace\", vocabulary=pickle.load(open(feature_path_P, \"rb\")))\n",
    "# Load TfidfTransformer\n",
    "tfidftransformer_path_P = 'P_N\\\\tfidftransformerP.pkl'\n",
    "tfidftransformer_P = pickle.load(open(tfidftransformer_path_P, \"rb\"))\n",
    "\n",
    "\n",
    "model_P_N = pickle.load(open(\"P_N\\\\mlp\", 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize  \n",
    "\n",
    "def clean_data1(text):\n",
    "  search = [\"ÿ£\",\"ÿ•\",\"ÿ¢\",\"ÿ©\",\"_\",\"-\",\"/\",\".\",\"ÿå\",\" Ÿà \",\" Ÿäÿß \",'\"',\"ŸÄ\",\"'\",\"Ÿâ\",\n",
    "              \"\\\\\",'\\n', '\\t','&quot;','?','ÿü','!']\n",
    "  replace = [\"ÿß\",\"ÿß\",\"ÿß\",\"Ÿá\",\" \",\" \",\"\",\"\",\"\",\" Ÿà\",\" Ÿäÿß\",\n",
    "               \"\",\"\",\"\",\"Ÿä\",\"\",' ', ' ',' ',' ? ',' ÿü ', ' ! ']\n",
    "  longation = re.compile(r'(.)\\1+')\n",
    "  subst = r\"\\1\\1\"\n",
    "  text = re.sub(longation, subst, text)\n",
    "  text = re.sub(r\"[a-zA-Z]\", '', text)\n",
    "  text = re.sub(r\"\\d+\", ' ', text)\n",
    "  text = re.sub(r\"\\n+\", ' ', text)\n",
    "  text = re.sub(r\"\\t+\", ' ', text)\n",
    "  text = re.sub(r\"\\r+\", ' ', text)\n",
    "  text = re.sub(r\"\\s+\", ' ', text)\n",
    "  text = re.sub(\"ÿßÿß+\", \"ÿß\", text)\n",
    "  text = re.sub(\"ŸàŸà+\", \"Ÿà\", text)\n",
    "  text = re.sub(\"ŸäŸä+\", \"Ÿä\", text)\n",
    "  text = re.sub(\"ŸáŸá+\", \"Ÿá\", text)\n",
    "  text = re.sub(\"ü§£ü§£+\",\"ü§£\", text)\n",
    "  text = re.sub(\"üòÇüòÇ+\",\"üòÇ\", text)\n",
    "  text = re.sub(\"üò≠üò≠+\",\"üò≠\", text)\n",
    "  text = re.sub(\"üò±üò±+\",\"üò±\", text)\n",
    "  text = re.sub(\"üò°üò°+\",\"üò°\", text)\n",
    "  text = re.sub(\"üòÄüòÄ+\",\"üòÄ\", text)\n",
    "\n",
    "\n",
    "  for i in range(0, len(search)):\n",
    "     text = text.replace(search[i], replace[i])\n",
    "    \n",
    "  text = text.strip()\n",
    " # text=corr.contextual_correct(text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_P(text):\n",
    "    sentence =  clean_data1(text)\n",
    "    tfidf = tfidftransformer_P.transform(loaded_vec_P.transform([sentence]))\n",
    "    predictions = model_P_N.predict(tfidf)\n",
    "    return predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"ŸÇÿ±ÿ±ÿ™ ŸÖŸàŸàŸàŸàŸàŸàÿ™ Ÿäÿ±ÿ≠ŸÖ ÿ¨ÿØŸÉ ŸÅŸáÿßŸÖ   \"\n",
    "\n",
    "model_P(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
